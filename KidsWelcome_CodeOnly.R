###############################################################
# Kids Welcome - Predicting child-friendliness                #
# of restaurant and food businesses from Yelp data            #
# Full code                                                   #
#                                                             #
# Timm Suess, 2015-11-15                                      #
# http://github.com/lordyo                                    #
###############################################################


# Load libraries ----------------------------------------------------------

# preprocessing
library(dplyr)
library(tidyjson)
library(tidyr)
library(stringr)
library(tm)

# modelling
library(caret)
library(foreach)
library(doParallel)
library(parallel)
library(pROC)
library(ggplot2)
library(RColorBrewer)


# custom functions --------------------------------------------------------

relevelr <- function(x, prefix) {
  # Take a factor variable vector and attach a prefix to its levels
  # Input:
  #  x = Factor variable
  #  prefix = character string used as prefix
  # Return:
  #  factor variable vector with prefixes
  
  levels(x) <- sub("^(.+$)", paste0(prefix, "\\1"), levels(x))
  x
}

Spread_bool <- function(df, varname) {
  # take a factor variable and dummify it (spread factors to boolean variables)
  # Input:
  #  df = a data frame with at least one factorial variable
  #  varname = name of the variable to be dummified (as quoted character string)
  # Return:
  #  df = data frame with variable replaced by dummified variables 
  
  library(dplyr)
  library(tidyr)
  
  df <- df %>% 
    mutate(truval = TRUE) %>% 
    spread_(varname, "truval", fill = FALSE) %>% 
    select(-`NA`)
  
  df
}

construct_cmdf <- function(cf_object, modelname) {
  # constructs a data frame of metrics from a confusion matrix object
  # Input:
  #  cf_object: a ConfusionMatrix object generated by caret::confusionMatrix
  #  modelname: the name of the tested model (quoted character string)
  # Return:
  #  df: a longformat data frame of model name, metric name and value
  
  df <- data.frame(model = modelname,
                   metric = names(cf_object$overall),
                   value = cf_object$overall,
                   stringsAsFactors = FALSE)
  
  rownames(df) <- NULL
  
  df2 <- data.frame(model = modelname,
                    metric = names(cf_object$byClass),
                    value = cf_object$byClass,
                    stringsAsFactors = FALSE)
  
  rownames(df2) <- NULL
  
  df <- bind_rows(df, df2)
  
  df
}


# NOT RUN: Import & indexing of business.json -----------------------------
# Processing business.json takes a long time (~8 hrs), load data instead

# Import lovally saved businesses data
filename <- "raw_data/yelp_academic_dataset_business.json"

# reformat newline separated json data (multiple entries) to comma separated json data (one entry) 
businesses <- sprintf("[%s]", paste(readLines(filename), collapse=","))

# create initial structure for json parsing -  this takes a couple of hours
business_array <- businesses %>% as.tbl_json() %>%  
  gather_array()

save(business_array, file = "processed_data/business_array")


# Construct non-nested business data (businesses_main) --------------------
# Creates a data frame "businesses_main" with the variables
# business_id, name, full_address, city, state, latitude/longitude,
# stars, review_count and open

load("processed_data/business_array")

businesses_main <- business_array %>% 
  spread_values(business_id = jstring("business_id"),
                name = jstring("name"),
                full_address = jstring("full_address"),
                city = jstring("city"),
                state = jstring("state"),
                latitude = jnumber("latitude"),
                longitude = jnumber("longitude"),
                stars = jnumber("stars"),
                review_count = jnumber("review_count"),
                open = jlogical("open"))  %>% 
  as.data.frame()

save(businesses_main, file = "processed_data/businesses_main.df")


# Construct business categories -------------------------------------------
# Creates a data frame "business_categories" with the business's ID and
# the respective category.

businesses_categories <- business_array %>% 
  spread_values(business_id = jstring("business_id")) %>% 
  enter_object("categories") %>% 
  gather_array() %>% 
  append_values_string("categories") %>% 
  select(business_id, categories) %>% 
  as.data.frame() %>% 
  spread(categories, categories)

# set NAs to FALSE, numbers to TRUE (dummyfication) 
businesses_categories[-(1)] <- !is.na(businesses_categories[-(1)])

save(businesses_categories, file = "processed_data/businesses_categories.df")


# Construct business attributes (Step 1) --------------------------------------
# Creates a data frame containing business attributes. There are nested
# attributes in the variables, i.e. it's not a tidy df yet. This df is needed
# for an interim step and will be replaced by a tidy one in a later step
# (hence "businesses_attributes_STEP1"). This code should be refactored.

businesses_attributes_step1 <- business_array %>% 
  spread_values(business_id = jstring("business_id")) %>% 
  enter_object("attributes") %>% gather_keys() %>%
  append_values_string() %>%
  as.data.frame() %>% 
  spread(key = key, value = string) %>% 
  select(-(document.id:array.index))

save(businesses_attributes_step1, file = "processed_data/businesses_attributes_step1.df")


# Get outcome variable (gfk) ----------------------------------------
# Constructs a data frame including business ID and the two "good for kids"
# variables. The two are originally named the same and are renamed to gfk1 and gfk2
# Consolidation happens in a later step.

good4kids <- businesses_attributes_step1[c(1, 21, 22)]

colnames(good4kids) <- c("business_id", "gfk1", "gfk2")

# remove business if both gfk1 and gfk2 are NA
good4kids <- good4kids[!is.na(good4kids[2]) | !is.na(good4kids[3]) , ]

good4kids <- mutate(good4kids, gfk1 = as.logical(gfk1), gfk2 = as.logical(gfk2))


# Join main, categories, outcome data frames ------------------------------
# Creates a data frame "good4kids" by joining the tables businesses_main, 
# businesses_categories and good4kids

good4kids <- right_join(businesses_main, good4kids, by = "business_id")
good4kids <- left_join(good4kids, businesses_categories, by = "business_id")

# Switch to long format by gathering categories
good4kids <- good4kids %>%
  select(-c(document.id, array.index)) %>% 
  gather(category, value, -c(business_id:gfk2)) %>% 
  filter(value == TRUE)


# Build hierarchical categories -------------------------------------------
# Attaches all hierarchical levels of a category to the good4kids data frame.
# The basis is a manually constructed hierarchy reference list based on the
# Yelp API, which is loaded in the first step.

yelp_categories <- read.csv("processed_data/yelp_categories_manual2.csv", na.strings = "")

# Calculate the depth (leafindex) of all categories
yelp_categories$leafindex <- 4 - (is.na(yelp_categories$Level1) + 
                                    is.na(yelp_categories$Level2) + 
                                    is.na(yelp_categories$Level3) + 
                                    is.na(yelp_categories$Level4) )

# Attach the deepest category name (this is the key for joining with
# the good4kids df)
yelp_categories <- yelp_categories %>% 
  mutate(Level1 = as.character(Level1), Level2 = as.character(Level2),
         Level3 = as.character(Level3), Level4 = as.character(Level4)) %>%
  mutate(leaf = ifelse(is.na(Level2), Level1, 
                       ifelse(is.na(Level3), Level2,
                              ifelse(is.na(Level4), Level3, Level4))))

# join categorical hierarchy to good4kids df
# (produces warning on character vs vector, which is fine)
good4kids <- left_join(good4kids, yelp_categories, by = c("category" = "leaf"))


# Unite outcome var, filter categories, spread categories ----------------------
# Reduce outcome variable to one, filter for Restaurant & Food businesses
# switch to wide format. New df is called good4kids_food.

# If gfk1 is NA, take gfk2, otherwise take gfk1
good4kids <- mutate(good4kids, gfk = ifelse(is.na(gfk1), gfk2, gfk1))

good4kids_food <- good4kids %>% 
  # filter for Food & Restaurant businesses (incl. subcategories)
  filter(Level1 == "Food" | Level1 == "Restaurants") %>%
  # If there is no subcategory (e.g. "Italian"), take the main category 
  # (e.g. "Restaurants")
  mutate(levelcat = ifelse(is.na(Level2), Level1, Level2)) %>% 
  select(-c(gfk1, gfk2, Level1, Level2, Level3, Level4, LevelNum, leafindex, category)) %>%
  # get rid of double entries
  unique() %>%
  # add "cat_" prefix to categories
  mutate(levelcat = paste0("cat_", levelcat)) %>%
  # wide format
  spread(levelcat, value, fill = FALSE)


# Get nested attributes ---------------------------------------------------
# Dives into the attributes index and constructs tables from the nested
# attributes: goodfor, ambience, diet, parking

# attribute "goodfor"
businesses_attributes_goodfor <- business_array %>% 
  spread_values(business_id = jstring("business_id")) %>% 
  enter_object("attributes") %>% 
  enter_object("Good For") %>% gather_keys() %>% 
  append_values_logical() %>% 
  as.data.frame() %>% 
  spread(key = key, value = logical)
colnames(businesses_attributes_goodfor) <- paste0("goodfor_", colnames(businesses_attributes_goodfor))

# attribute "ambience"
businesses_attributes_ambience <- business_array %>% 
  spread_values(business_id = jstring("business_id")) %>% 
  enter_object("attributes") %>% 
  enter_object("Ambience") %>% gather_keys() %>% 
  append_values_logical() %>% 
  as.data.frame() %>% 
  spread(key = key, value = logical)
colnames(businesses_attributes_ambience) <- paste0("ambience_", colnames(businesses_attributes_ambience))

# attribute "dietary restrictions"
businesses_attributes_diet <- business_array %>% 
  spread_values(business_id = jstring("business_id")) %>% 
  enter_object("attributes") %>% 
  enter_object("Dietary Restrictions") %>% gather_keys() %>% 
  append_values_logical() %>% 
  as.data.frame() %>% 
  spread(key = key, value = logical)
colnames(businesses_attributes_diet) <- paste0("diet_", colnames(businesses_attributes_diet))

# attribute "parking"
businesses_attributes_parking <- business_array %>% 
  spread_values(business_id = jstring("business_id")) %>% 
  enter_object("attributes") %>% 
  enter_object("Parking") %>% gather_keys() %>% 
  append_values_logical() %>% 
  as.data.frame() %>% 
  spread(key = key, value = logical)
colnames(businesses_attributes_parking) <- paste0("parking_", colnames(businesses_attributes_parking))

# top level attributes; this replaces the businesses_attributes_step1 df
businesses_attributes <- business_array %>% 
  spread_values(business_id = jstring("business_id")) %>% 
  enter_object("attributes") %>% gather_keys() %>%
  append_values_string() %>%
  as.data.frame() %>% 
  spread(key = key, value = string) %>% 
  select(-(document.id:array.index)) %>% 
  select(-matches("Good for Kids"))


# cleanup attributes --------------------------------------------------------
# various minor cleaning and renaming steps

# change all variables in businesses_attributes to conform to R naming standards
# (no special chars except underscore, all lower caps)
atnames <- colnames(businesses_attributes)
atnames <- gsub(pat = "\\W", rep="_", atnames)
atnames <- paste0("attr_", tolower(atnames))
atnames[1] <- "business_id"
colnames(businesses_attributes) <- atnames
rm(atnames)

# set character NAs to real NAs
businesses_attributes[businesses_attributes == "NA"] <- NA

# set classes (logical, factor)
businesses_attributes <- businesses_attributes %>% 
  mutate_each(funs(as.logical), -business_id, -attr_ages_allowed, -attr_alcohol,
              -attr_attire, -attr_byob_corkage, -attr_noise_level,
              -attr_price_range, -attr_smoking, -attr_wi_fi) %>% 
  mutate_each(funs(factor), business_id, attr_ages_allowed, attr_alcohol, 
              attr_attire, attr_byob_corkage, attr_noise_level,
              attr_price_range, attr_smoking, attr_wi_fi)

# add prefixes to factor variables (uses the custom function "relevelr()")
businesses_attributes$attr_ages_allowed <- relevelr(businesses_attributes$attr_ages_allowed,"attr_ages_allowed_")
businesses_attributes$attr_alcohol <-relevelr(businesses_attributes$attr_alcohol,"attr_alcohol_")
businesses_attributes$attr_attire <- relevelr(businesses_attributes$attr_attire,"attr_attire_")
businesses_attributes$attr_byob_corkage <- relevelr(businesses_attributes$attr_byob_corkage,"attr_byob_corkage_")
businesses_attributes$attr_smoking <- relevelr(businesses_attributes$attr_smoking,"attr_smoking_")
businesses_attributes$attr_wi_fi <- relevelr(businesses_attributes$attr_wi_fi,"attr_wi_fi_")

# dummify factor variables (uses the custom function "spread_bool())
businesses_attributes <- businesses_attributes %>% 
  Spread_bool("attr_ages_allowed") %>% 
  Spread_bool("attr_alcohol") %>% 
  Spread_bool("attr_attire") %>% 
  Spread_bool("attr_byob_corkage") %>% 
  Spread_bool("attr_smoking") %>% 
  Spread_bool("attr_wi_fi") %>% 
  mutate(attr_price_range = ifelse(!is.na(attr_price_range), 
                                   paste0("pricerange_", attr_price_range), NA)) %>%
  Spread_bool("attr_price_range") %>%
  mutate(attr_noise_level = ifelse(!is.na(attr_noise_level), 
                                   paste0("noise_", attr_noise_level), NA)) %>%
  Spread_bool("attr_noise_level")


# Stitch attribute tables together ----------------------------------------
# Creates one big attributes df with all lower-level attributes
# (main attributes, goodfor, ambience, diet, parking)

businesses_attributes <- left_join(businesses_attributes, 
                                   businesses_attributes_goodfor, 
                                   by = c("business_id" = "goodfor_business_id"))

businesses_attributes <- left_join(businesses_attributes,
                                   businesses_attributes_ambience, 
                                   by = c("business_id" = "ambience_business_id"))

businesses_attributes <- left_join(businesses_attributes,
                                   businesses_attributes_diet, 
                                   by = c("business_id" = "diet_business_id"))

businesses_attributes <- left_join(businesses_attributes,
                                   businesses_attributes_parking, 
                                   by = c("business_id" = "parking_business_id"))

businesses_attributes <- select(businesses_attributes, -c(ambience_document.id, ambience_array.index,
                                                          goodfor_document.id, goodfor_array.index,
                                                          diet_document.id, diet_array.index,
                                                          parking_document.id, parking_array.index))

save(businesses_attributes, file = "processed_data/businesses_attributes.df")

# attach attributes to good4kids_food table
good4kids_food <- left_join(good4kids_food, businesses_attributes, by = "business_id")


# Create summaries overview of all variables ------------------------------
# Since we're dealing with a lot of variables, we need a more comfortable
# way to summarise them. The following code creates a summary of summary()s,
# which will later be used to throw out variables with 0 variance

sumdata <- good4kids_food[10:ncol(good4kids_food)]

na_summary <- as.data.frame(summary(sumdata))

na_summary <- na_summary %>% 
  separate(Freq, into = c("key", "value"), sep = " *:", convert = TRUE) %>% 
  filter(!is.na(key)) %>% 
  spread(key, value) %>% 
  select(Var2, `NA's`, `FALSE`, `TRUE`) %>% 
  mutate(`NA's`=as.integer(`NA's`), `TRUE` = as.integer(`TRUE`), `FALSE`=as.integer(`FALSE`))

na_summary[2:4] <- round(na_summary[2:4] / nrow(sumdata),3)

na_summary <- arrange(na_summary, `TRUE`)


# Get cities --------------------------------------------------------------
# use two k-means clustering passes to attribute the businesses to one of 
# the 10 cities in the Yelp set. This is better than using the official
# city names, which are too detailed and possibly ambiguous.
# Note that this is a crude, unelegant trial-and-error approach, but since
# we know what the 10 cities are, it does the job well. For different data sets
# (with other cities), this needs to be finetuned manually.

# specific seed is needed here, or the result will differ
set.seed(777)

latlon <- good4kids_food[c("latitude", "longitude")]

# first kmeans pass. 
kfit <- kmeans(latlon, 10)

# this results in 8 correct and 2 false cluster centers (8, 10)
# (city names found manually via Google Maps)
# 1        1 33.38324 -111.8569329  Phoenix (east)
# 2        2 33.57034 -112.1230991  Phoenix (northwest)
# 3        3 36.15960 -115.2419689  Las Vegas (west)
# 4        4 40.11080  -88.2289851  Champaign-Urbana
# 5        5 45.50971  -73.6030370  Montreal
# 6        6 43.08005  -89.4033990  Madison
# 7        7 35.18563  -80.8286818  Charlotte
# 8        8 40.87803  -80.0545346  (unclear)
# 9        9 36.09558 -115.1237402  Las Vegas (south)
# 10      10 54.37946   -0.5718344  (unclear)

latlon <- data.frame(city = good4kids_food[c("business_id", "city", "latitude", "longitude")],
                     cluster = kfit$cluster)

# get locations from ambiguous clusters 8 and 10
latlon810 <- latlon %>% 
  filter(cluster == 8 | cluster == 10) %>% 
  select(business_id = city.business_id, latitude = city.latitude, longitude = city.longitude)

# again, this specific seed is necessary to get the results
set.seed(999)

# second k-means pass, only for clusters 8 and 10
kfit810 <- kmeans(latlon810[2:3], 5)
# this results in 5 clusters (city names found manually via Google Maps):
# 1       1 55.95575  -3.327115 Edinburgh (west)
# 2       2 55.94964  -3.190991 Edinburgh (center)
# 3       3 49.00299   8.408713 Karlsruhe
# 4       4 40.43611 -79.977053 Pittsburg
# 5       5 43.46028 -80.507283 Waterloo

latlon810$cluster <- kfit810$cluster

latlon810$cluster <- latlon810$cluster + 10

# attach city names to cluster centers
cities <- data.frame(cluster = c(1:15), city.name = c("Phoenix", "Phoenix", "Las Vegas",
                                                      "Urbana-Champaign", "Montreal",
                                                      "Madison", "Charlotte", NA, "Las Vegas",
                                                      NA, "Edinburgh", "Edinburgh", "Karlsruhe",
                                                      "Pittsburgh", "Waterloo"))

latlon810 <- select(latlon810, business_id, cluster)

latlon <- left_join(latlon, latlon810, by = (c("city.business_id" = "business_id")))

# attach city names to latlon df, clean
latlon <- latlon %>% 
  mutate(cluster = ifelse(is.na(cluster.y), cluster.x, cluster.y)) %>% 
  left_join(cities, by = "cluster") %>% 
  select(business_id = city.business_id, city.name) %>%
  mutate(city.name = paste0("city_", city.name)) %>% 
  mutate(truval = TRUE) %>%
  spread(city.name, "truval", fill = FALSE)

# attach city info to good4kids_food df
good4kids_food <- left_join(good4kids_food, latlon, by = "business_id")


# Clean up unnecessary variables ------------------------------------------

# Remove any attribute with 0% TRUE values (from the na_summary table)
zero_true <- na_summary %>% 
  filter(`TRUE` == 0) %>% 
  select(Var2) %>%
  mutate(Var2 = as.character(Var2)) 

# get vector of 0% TRUE var names
zero_true <- zero_true$Var2
zero_true <- str_trim(zero_true)

# Start constructing final prediction table "gfk_clean"
gfk_clean <- good4kids_food %>% 
  select(-one_of(zero_true))

# get rid of pecific geographical information, except cities

gfk_clean <- select(gfk_clean, -(full_address:longitude))

# get rid of the following attributes:
# * "open": a potentially confounding outcome variable not useful for
#           a prediction engine
# * "ambience", "good_for", "dietary_restrictions", "parking"
#   (these are the original nested variable names and have no content)
# * "hair types specialized in", "accepts insurance"
#   (attributes not applicable to restaurant / food businesses)
# * "payment types": already covered by "accepts credit cards"

gfk_clean <- select(gfk_clean, -open, -attr_ambience, -attr_good_for, 
                    -attr_hair_types_specialized_in,-attr_dietary_restrictions, 
                    -attr_parking, -attr_payment_types, -attr_accepts_insurance)


# Get most important terms in business names ------------------------------
# Uses natural language processing to create a document-term matrix of the 
# words contained in at least 0.1% of all businesses (except stop words)

venues <- data.frame(business_id = gfk_clean$business_id, content = gfk_clean$name)

# create Corpus
m <- list(id = "business_id", content = "content")
myReader <- readTabular(mapping = m)
venue_corpus <- Corpus(DataframeSource(venues),
                       readerControl = list(reader = myReader, language = "en"))

venue_corpus <- tm_map(venue_corpus, stripWhitespace) #white spaces
venue_corpus <- tm_map(venue_corpus, content_transformer(tolower))  #lower case
venue_corpus <- tm_map(venue_corpus, removeWords, stopwords("english")) #stop words
venue_corpus <- tm_map(venue_corpus, removePunctuation, 
                       preserve_intra_word_dashes = FALSE) #regular punctuation
venue_corpus <- tm_map(venue_corpus, removeNumbers) # numbers
venue_corpus <- tm_map(venue_corpus, stemDocument) # stemming

# Create DTM with words longer than 3 and shorter than 15 characters
venue_dtm <- DocumentTermMatrix(venue_corpus, control = list(wordLengths = c(3,15)))

venue_dtm <- data.frame(as.matrix(venue_dtm), stringsAsFactors = TRUE)

venue_dtm$business_id <- rownames(venue_dtm)
venue_dtm <- select(venue_dtm, business_id, everything())

# count number of times the word appears
venue_dtm_nobiz <- select(venue_dtm, -business_id)
venue_cs <- data.frame(term = colnames(venue_dtm_nobiz), 
                       csum = colSums(venue_dtm_nobiz))

# create shortlist vector of variable names with more than 20 counts
venue_dtm_prunelist <- venue_cs$term[venue_cs$csum > 20]
venue_dtm_prunelist <- as.character(venue_dtm_prunelist)

# reduce dtm to shortlisted terms
venue_dtm_pruned <- venue_dtm %>% 
  select(business_id, one_of(venue_dtm_prunelist))

# clean up
termlist <- colnames(venue_dtm_pruned)
termlist <- paste0("term_", tolower(termlist))
termlist[1] <- "business_id"
colnames(venue_dtm_pruned) <- termlist

# attach shortlisted dtm to gfk_clean
gfk_clean <- left_join(gfk_clean, venue_dtm_pruned, by = "business_id")


# final cleanup, conclude preprocessing-----------------------------------------

# clean up variable names to R's naming standards (again)
atnames <- colnames(gfk_clean)
atnames <- gsub(pat = "\\W+", rep="_", atnames)
atnames <- gsub(pat = "_$", rep="", atnames)
atnames <- tolower(atnames)
colnames(gfk_clean) <- atnames

# code binary variables as 0/1 instead of FALSE/TRUE
gfk_clean <- gfk_clean %>% 
  mutate_each(funs(as.integer), -business_id, -name, -stars, -review_count) %>% 
  mutate(review_count_log = log(review_count))

save(gfk_clean, file = "processed_data/gfk_clean.df")

# remove unnecessary variables
rm(business_array, businesses_attributes, businesses_main, businesses_attributes_ambience,
   businesses_attributes_goodfor, businesses_attributes_step1, businesses_categories,
   cities, good4kids, good4kids_food, latlon, latlon810, na_summary, sumdata, venue_cs,
   venue_dtm, venue_dtm_nobiz, venue_dtm_pruned, yelp_categories, kfit, kfit810,
   venue_dtm_prunelist, zero_true, relevelr, Spread_bool, businesses_attributes_parking,
   businesses_attributes_diet, atnames, m, termlist, venue_corpus, myReader, venues)

# clean up memory
gc()

# this concludes the pre-processing steps - the predictor data frame stands.


# Create training, validating, testing set -------------------------------------
# prepare and split data for training , validation, testing (60/20/20)

gfk_modelbase <- select(gfk_clean, -business_id, -name)

set.seed(333)

inTrain1 <- createDataPartition(y = gfk_modelbase$gfk,
                                p = 0.8,
                                list = FALSE)

training1 <- gfk_modelbase[ inTrain1, ]
testing   <- gfk_modelbase[-inTrain1, ]

inTrain2 <- createDataPartition(y = training1$gfk,
                                p = 0.75,
                                list = FALSE)

training2  <- training1[ inTrain2, ]
validating <- training1[-inTrain2, ] 

training <- training2

rm(training1, training2, inTrain1, inTrain2)

save(training, file = "processed_data/training.df")
save(testing, file = "processed_data/testing.df")
save(validating, file = "processed_data/validating.df")


# Impute NAs with median -------------------------------------------------------

# construct repeatable imputer from training set
imputer <- preProcess(training, method = "medianImpute")

# apply imputer to training set itself
training_imp <- predict(imputer, newdata = training)


# Prepare outcome variable for training -----------------------------------

# unnecessary relic from when I used subsamples for faster processing
training_s <- training_imp

# outcome variable gfk as factor (required for classification)
training_s <- training_s %>% 
  mutate(gfk = ifelse(gfk == 0, "no", "yes")) %>% 
  mutate(gfk = as.factor(gfk))

# set predictor and outcome names for flexibility
outcomeName <- 'gfk'
predictorsNames <- names(training_s)[names(training_s) != "gfk"]

save(training_imp, file = "processed_data/training_imp.df")


# NOT RUN: Training: Random Forest ---------------------------------------------
# this takes a *lot* of time on a single machine

# set control parameters
objControl_rf <- trainControl(classProbs = TRUE,
                              summaryFunction = twoClassSummary,
                              nodesize = 120)

# turn on parallel processing
corenum = detectCores()
registerDoParallel(cores=corenum)

# train model - this takes a couple of hours
modfit_rf <- train(y = training_s[ , outcomeName],
                   x = training_s[ , predictorsNames],
                   method = "parRF",
                   metric = "ROC",
                   trControl = objControl_rf)

save(modfit_rf, file = "processed_data/modfit_rf.caret")


# NOT RUN: Training: LogitBoost ---------------------------------------------
# this takes some time on a single machine

# set control parameters
objControl_logitboost <- trainControl(classProbs = TRUE,
                                      summaryFunction = twoClassSummary)

# train model - takes a lot of time
modfit_logitboost <- train(y = training_s[ , outcomeName],
                           x = training_s[ , predictorsNames],
                           method = "LogitBoost",
                           metric = "ROC",
                           trControl = objControl_logitboost)

save(modfit_logitboost, file = "processed_data/modfit_logitboost.caret")


# Training: Neural Net ---------------------------------------------

# set control parameters
objControl_nnet <- trainControl(classProbs = TRUE,
                                summaryFunction = twoClassSummary)

# train model
modfit_nnet <- train(y = training_s[ , outcomeName],
                     x = training_s[ , predictorsNames],
                     method="nnet",
                     metric = "ROC",
                     trControl = objControl_nnet)

save(modfit_nnet, file = "processed_data/modfit_nnet")


# Explore in-sample models ------------------------------------------------

load("processed_data/modfit_rf.caret")
load("processed_data/modfit_logitboost.caret")
load("processed_data/modfit_nnet")

# RF
summary(modfit_rf)
varImp(modfit_rf)
varImpPlot(modfit_rf)
print(modfit_rf)
plot(modfit_rf)

# logitboost
summary(modfit_logitboost)
print(modfit_logitboost)
plot(modfit_logitboost)

# nnet
summary(modfit_nnet)
print(modfit_nnet)


# Validate: Prepare data -------------------------------------------------

# impute using imputer
validating_imp <- predict(imputer, newdata = validating)

validating_imp <- validating_imp %>% 
  mutate(gfk = ifelse(gfk == 0, "no", "yes")) %>% 
  mutate(gfk = as.factor(gfk))


# Validate: Random Forest -------------------------------------------------

# apply RF model to validating data (absolute predictions)
val_rf_raw <- predict(object=modfit_rf,
                      validating_imp[ , predictorsNames], type='raw')

# show results
confusionMatrix(val_rf_raw, validating_imp$gfk, positive = "yes")

# apply RF model to validating data (probabilistic predictions)
val_rf_prob <- predict(object=modfit_rf, validating_imp[ ,predictorsNames],
                       type='prob')

# construct and show ROC curve
auc_rf <- roc(ifelse(validating_imp[ , outcomeName]=="yes",1,0), val_rf_prob[[2]])
print(auc_rf$auc)
plot(auc_rf)


# Validate: LogitBoost ----------------------------------------------------

# apply LB model to validating data (absolute predictions)
val_logitboost_raw <- predict(object=modfit_logitboost, validating_imp[ , predictorsNames], type='raw')

# show results
confusionMatrix(val_logitboost_raw, validating_imp$gfk, positive = "yes")

# apply LB model to validating data (probabilistic predictions)
val_logitboost_prob <- predict(object=modfit_logitboost, validating_imp[ , predictorsNames], type='prob')

# construct and show ROC curve
auc_logitboost <- roc(ifelse(validating_imp[ , outcomeName]=="yes",1,0), val_logitboost_prob[[2]])
print(auc_logitboost$auc)
plot(auc_logitboost)


# Validating: Neural Net --------------------------------------------------

# apply NN model to validating data (absolute predictions)
val_nnet_raw <- predict(object=modfit_nnet, validating_imp[ , predictorsNames], type='raw')

# show results
confusionMatrix(val_nnet_raw, validating_imp$gfk, positive = "yes")

# apply NN model to validating data (probabilistic predictions)
val_nnet_prob <- predict(object=modfit_nnet, validating_imp[ , predictorsNames], type='prob')

# construct and show ROC curve
auc_nnet <- roc(ifelse(validating_imp[ , outcomeName]=="yes",1,0), val_nnet_prob[[2]])
print(auc_nnet$auc)
plot(auc_nnet)


# Train General Additive Model ----------------------------------------
# based on the three validated models RF, LB, NN

pred_gam <- data.frame(val_rf_raw, val_logitboost_raw, val_nnet_raw, 
                       gfk = validating_imp$gfk)

objControl_gam <- trainControl(classProbs = TRUE, 
                               summaryFunction = twoClassSummary)

modfit_gam <- train(gfk ~ .,
                    data = pred_gam,
                    method="gam",
                    metric = "ROC",
                    trControl = objControl_gam)

save(modfit_gam, file = "processed_data/modfit_gam.caret")

# Explore in-sample GAM model

summary(modfit_gam)
print(modfit_gam)
plot(modfit_gam)


# Prepare Testing data ----------------------------------------------------

testing_imp <- predict(imputer, newdata = testing)

testing_imp <- testing_imp %>% 
  mutate(gfk = ifelse(gfk == 0, "no", "yes")) %>% 
  mutate(gfk = as.factor(gfk))


# Test General Additive Model ---------------------------------------------

# apply GAM model to testing data (absolute predictions)
test_gam_raw <- predict(object=modfit_gam, testing_imp[ , predictorsNames],
                        type='raw')

# get results
confusionMatrix(test_gam_raw, testing_imp$gfk, positive = "yes")

# apply GAM model to testing data (probabilistic predictions)
test_gam_prob <- predict(object=modfit_gam, testing_imp[ , predictorsNames], type='prob')

# construct and show ROC curve
auc_gam <- roc(ifelse(testing_imp[ , outcomeName]=="yes",1,0), test_gam_prob[[2]])
print(auc_gam$auc)
plot(auc_gam)


# Test RF model against validating+testing data ---------------------------

# construct validating + testing set (40%)
valtest_imp <- bind_rows(validating_imp, testing_imp)
valtest_imp <- as.data.frame(valtest_imp)

# apply RF (absolute)
valtest_rf_raw <- predict(object=modfit_rf, valtest_imp[ , predictorsNames], type='raw')

# get results
confusionMatrix(valtest_rf_raw, valtest_imp$gfk, positive = "yes")

# applfy RF (probabilistic)
valtest_rf_prob <- predict(object=modfit_rf, valtest_imp[ , predictorsNames], type='prob')

# get ROC curve
auc_rf <- roc(ifelse(valtest_imp[ , outcomeName]=="yes",1,0), valtest_rf_prob[[2]])
print(auc_rf$auc)
plot(auc_rf)


# save modelling data for re-use ------------------------------------------
save(validating_imp, file = "processed_data/validating_imp.df")
save(testing_imp, file = "processed_data/testing_imp.df")
save(auc_rf, file = "processed_data/auc_rf.roc")
save(auc_logitboost, file = "processed_data/auc_logitboost.roc")
save(auc_nnet, file = "processed_data/auc_nnet.roc")
save(auc_gam, file = "processed_data/auc_gam.roc")
save(val_rf_raw, file = "processed_data/val_rf_raw.confusionmatrix")
save(val_logitboost_raw, file = "processed_data/val_logitboost_raw.confusionmatrix")
save(val_nnet_raw, file = "processed_data/val_nnet_raw.confusionmatrix")
save(test_gam_raw, file = "processed_data/test_gam_raw.confusionmatrix")


# Explore 25 most important variables from the RF model ------------------------

rf_varimp <- as.data.frame(modfit_rf$finalModel$importance)
rf_varimp$Predictor <- row.names(rf_varimp)
row.names(rf_varimp) <- NULL

rf_varimp <- rf_varimp %>% 
  arrange(desc(MeanDecreaseGini)) %>% 
  slice(1:25) %>% 
  `[[`(2)

rf_varimp

# get directions of 25 most important variables from training contingency tables 
gfk_top25 <- select(training_imp, one_of(c("gfk", rf_varimp))) %>% 
  gather(Variable, Value, attr_alcohol_full_bar:attr_outdoor_seating) %>% 
  select(Variable, Value, gfk) %>%
  filter(Variable != "stars" & Variable !="review_count" & 
           Variable != "review_count_log" & Value != 2) %>% 
  count(Variable, Value, gfk) %>%
  spread(Value, n, fill = 0) %>% 
  select(Variable, gfk, attr0 = `0`, attr1 = `1`) %>% 
  mutate(gfk2 = gfk) %>% 
  spread(gfk, attr0, fill = 0) %>%
  select(Variable, gfk2, attr1, a0g0 = `0`, a0g1 = `1`) %>% 
  spread(gfk2, attr1, fill = 0) %>% 
  select(Variable, a0g0, a0g1, a1g0 = `0`, a1g1 = `1`) %>% 
  group_by(Variable) %>% 
  summarise_each(funs(sum)) %>% 
  mutate(direction = sign(a0g0/(a0g0+a1g0)-a0g1/(a0g1+a1g1))) %>% 
  mutate(direction = ifelse(direction == 1, "+ positive", "- negative")) %>% 
  select(Predictor = Variable, Direction = direction)

gfk_top25


# Show model comparison and ROC curves ------------------------------------
# display an overview of model metrics in comparison
# show a comparison of ROC curves

# get confusion matrices from all 4 models
cm_v_rf <- confusionMatrix(val_rf_raw, validating_imp$gfk, positive = "yes")
cm_v_lb <- confusionMatrix(val_logitboost_raw, validating_imp$gfk, positive = "yes")
cm_v_nn <- confusionMatrix(val_nnet_raw, validating_imp$gfk, positive = "yes")
cm_t_gam <- confusionMatrix(test_gam_raw, testing_imp$gfk, positive = "yes")

# get AUROC values from all models
auc_values <- data.frame(model = c("Random Forest", "LogitBoost", "Neural Net", "GAM"),
                         metric = rep("AUROC",4),
                         value = c(as.numeric(auc_rf$auc),
                                   as.numeric(auc_logitboost$auc),
                                   as.numeric(auc_nnet$auc),
                                   as.numeric(auc_gam$auc)),
                         stringsAsFactors = FALSE) 

# construct data frame of all metrics, using the custom function construct_cmdf()
cm_df <- construct_cmdf(cm_v_rf, "Random Forest") %>% 
  bind_rows(construct_cmdf(cm_v_lb, "LogitBoost")) %>% 
  bind_rows(construct_cmdf(cm_v_nn, "Neural Net")) %>% 
  bind_rows(construct_cmdf(cm_t_gam, "GAM")) %>%
  bind_rows(auc_values) %>%
  # keep Accuracy, Kappa, Sensitivity, Specificity, Bal.Acc. and AUROC
  filter(metric == "Accuracy" |
           metric == "Kappa" |
           metric == "Sensitivity" |
           metric == "Specificity" |
           metric == "Balanced Accuracy" |
           metric == "AUROC") %>% 
  # wide format
  spread(metric, value) %>% 
  select(Model = model, Accuracy, Sensitivity, Specificity, Kappa,
         Balanced_Accuracy = `Balanced Accuracy`, AUROC) %>% 
  arrange(desc(AUROC))

# cleanup
row.names(cm_df) <- cm_df$Model
cm_df <- select(cm_df, -Model)

# show results
cm_df
